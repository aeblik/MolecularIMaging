{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from segment_anything import sam_model_registry, SamPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dave-\\anaconda3\\envs\\MoIm\\Lib\\site-packages\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Load SAM Model\n",
    "model_type = \"vit_h\"  # Model type can be \"vit_h\", \"vit_l\", or \"vit_b\"\n",
    "sam = sam_model_registry[model_type](checkpoint=\"./sam_vit_h_4b8939.pth\")\n",
    "#sam = sam_model_registry[model_type](checkpoint=\"./sam_vit_l_0b3195.pth\")\n",
    "#sam = sam_model_registry[model_type](checkpoint=\"./sam_vit_b_01ec64.pth\")\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a unique identifier from a filename using a regular expression pattern.\n",
    "    \"\"\"\n",
    "    match = re.match(r\"^([A-D]\\d+_\\d+).*_(\\d{3})\\.tif$\", filename)\n",
    "    return f\"{match.group(1)}_{match.group(2)}\" if match else None\n",
    "\n",
    "def split_mask(mask: np.array) -> tuple:\n",
    "    \"\"\"\n",
    "    Splits a mask into two regions: head (first one-third) and tail (remaining two-thirds).\n",
    "    \"\"\"\n",
    "    split_index = mask.shape[1] // 3  # Compute the index for one-third of the image width\n",
    "\n",
    "    head_mask = np.zeros_like(mask)  # Mask for the head (first one-third)\n",
    "    tail_mask = np.zeros_like(mask)  # Mask for the tail (remaining two-thirds)\n",
    "\n",
    "    head_mask[:, :split_index] = mask[:, :split_index]  # First one-third (head)\n",
    "    tail_mask[:, split_index:] = mask[:, split_index:]  # Remaining two-thirds (tail)\n",
    "\n",
    "    return head_mask, tail_mask\n",
    "\n",
    "def process_images(input_dir: str, output_dir: str, \n",
    "                   save_masks: bool = True, save_masked_images: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes pairs of 'Phase Contrast' and 'GFP' images to generate a binary mask using SAM, \n",
    "    calculate GFP intensity within the mask, and save mask and masked images for head, tail, and whole fish.\n",
    "    \"\"\"\n",
    "    # Create subdirectories for saving masks and masked images\n",
    "    phase_mask_dir = os.path.join(output_dir, \"phase_masks\")\n",
    "    head_mask_dir = os.path.join(output_dir, \"head_masks\")\n",
    "    tail_mask_dir = os.path.join(output_dir, \"tail_masks\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    if save_masks:\n",
    "        os.makedirs(phase_mask_dir, exist_ok=True)\n",
    "        os.makedirs(head_mask_dir, exist_ok=True)\n",
    "        os.makedirs(tail_mask_dir, exist_ok=True)\n",
    "\n",
    "    phase_contrast_files = {}\n",
    "    gfp_files = {}\n",
    "\n",
    "    for file_name in os.listdir(input_dir):\n",
    "        if file_name.endswith(\".tif\"):\n",
    "            identifier = get_identifier(file_name)\n",
    "            if identifier:\n",
    "                if \"Phase Contrast\" in file_name:\n",
    "                    phase_contrast_files[identifier] = file_name\n",
    "                elif \"GFP\" in file_name:\n",
    "                    gfp_files[identifier] = file_name\n",
    "\n",
    "    gfp_intensity_results = []\n",
    "\n",
    "    for i, (identifier, phase_file) in enumerate(phase_contrast_files.items()):\n",
    "        if identifier in gfp_files:\n",
    "            try:\n",
    "                phase_path = os.path.join(input_dir, phase_file)\n",
    "                gfp_path = os.path.join(input_dir, gfp_files[identifier])\n",
    "                \n",
    "                # Load Phase Contrast image\n",
    "                phase_image = Image.open(phase_path)\n",
    "                phase_np = np.array(phase_image, dtype=np.uint16)\n",
    "                phase_np = (phase_np / phase_np.max() * 255).astype(np.uint8)\n",
    "                phase_rgb = np.stack([phase_np] * 3, axis=-1)\n",
    "\n",
    "                predictor.set_image(phase_rgb)\n",
    "\n",
    "                # Generate primary mask\n",
    "                input_point = np.array([[phase_rgb.shape[1] // 2, phase_rgb.shape[0] // 2]])\n",
    "                input_label = np.array([1])\n",
    "                masks, scores, _ = predictor.predict(\n",
    "                    point_coords=input_point,\n",
    "                    point_labels=input_label,\n",
    "                    multimask_output=False\n",
    "                )\n",
    "                primary_mask = masks[0]\n",
    "\n",
    "                # Save whole fish mask\n",
    "                if save_masks:\n",
    "                    mask_output_path = os.path.join(phase_mask_dir, f\"mask_{os.path.splitext(phase_file)[0]}.png\")\n",
    "                    Image.fromarray((primary_mask * 255).astype(np.uint8)).save(mask_output_path)\n",
    "\n",
    "                # Split mask into head and tail\n",
    "                head_mask, tail_mask = split_mask(primary_mask)\n",
    "\n",
    "                # Save head and tail masks\n",
    "                if save_masks:\n",
    "                    head_mask_output_path = os.path.join(head_mask_dir, f\"head_mask_{os.path.splitext(phase_file)[0]}.png\")\n",
    "                    tail_mask_output_path = os.path.join(tail_mask_dir, f\"tail_mask_{os.path.splitext(phase_file)[0]}.png\")\n",
    "                    Image.fromarray((head_mask * 255).astype(np.uint8)).save(head_mask_output_path)\n",
    "                    Image.fromarray((tail_mask * 255).astype(np.uint8)).save(tail_mask_output_path)\n",
    "\n",
    "                # Load GFP image\n",
    "                gfp_image = Image.open(gfp_path)\n",
    "                gfp_np = np.array(gfp_image, dtype=np.uint16)\n",
    "\n",
    "                # Calculate GFP intensities for whole fish, head, and tail\n",
    "                gfp_values_within_fish = gfp_np[primary_mask > 0]\n",
    "                mean_gfp_intensity = gfp_values_within_fish.mean() if gfp_values_within_fish.size > 0 else 0\n",
    "                total_gfp_intensity = gfp_values_within_fish.sum()\n",
    "\n",
    "                gfp_values_within_head = gfp_np[head_mask > 0]\n",
    "                mean_gfp_head = gfp_values_within_head.mean() if gfp_values_within_head.size > 0 else 0\n",
    "                total_gfp_head = gfp_values_within_head.sum()\n",
    "\n",
    "                gfp_values_within_tail = gfp_np[tail_mask > 0]\n",
    "                mean_gfp_tail = gfp_values_within_tail.mean() if gfp_values_within_tail.size > 0 else 0\n",
    "                total_gfp_tail = gfp_values_within_tail.sum()\n",
    "\n",
    "                # Save masked GFP images\n",
    "                if save_masked_images:\n",
    "                    gfp_masked = gfp_np * primary_mask\n",
    "                    head_masked = gfp_np * head_mask\n",
    "                    tail_masked = gfp_np * tail_mask\n",
    "\n",
    "                    gfp_output_path = os.path.join(output_dir, f\"masked_{identifier}.png\")\n",
    "                    head_output_path = os.path.join(output_dir, f\"masked_head_{identifier}.png\")\n",
    "                    tail_output_path = os.path.join(output_dir, f\"masked_tail_{identifier}.png\")\n",
    "\n",
    "                    Image.fromarray(gfp_masked.astype(np.uint16)).save(gfp_output_path)\n",
    "                    Image.fromarray(head_masked.astype(np.uint16)).save(head_output_path)\n",
    "                    Image.fromarray(tail_masked.astype(np.uint16)).save(tail_output_path)\n",
    "\n",
    "                # Append GFP intensity results\n",
    "                gfp_intensity_results.append({\n",
    "                    \"Identifier\": identifier,\n",
    "                    \"Mean_GFP_Intensity\": mean_gfp_intensity,\n",
    "                    \"Total_GFP_Intensity\": total_gfp_intensity,\n",
    "                    \"Mean_GFP_Head\": mean_gfp_head,\n",
    "                    \"Total_GFP_Head\": total_gfp_head,\n",
    "                    \"Mean_GFP_Tail\": mean_gfp_tail,\n",
    "                    \"Total_GFP_Tail\": total_gfp_tail\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {identifier}: {e}\")\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(phase_contrast_files)} image pairs.\")\n",
    "\n",
    "    results_df = pd.DataFrame(gfp_intensity_results)\n",
    "    csv_output_path = os.path.join(output_dir, \"gfp_intensity_results.csv\")\n",
    "    results_df.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    print(f\"Results saved to {csv_output_path}\") \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "input_dir = r\"C:\\Users\\dave-\\OneDrive - ZHAW\\HS24\\MoIm\\MolecularIMaging\\Images\\test_images\"\n",
    "output_dir = r\"C:\\Users\\dave-\\OneDrive - ZHAW\\HS24\\MoIm\\MolecularIMaging\\Images\\test_images\\output_hugeSAM\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5/8 image pairs.\n",
      "Results saved to C:\\Users\\dave-\\OneDrive - ZHAW\\HS24\\MoIm\\MolecularIMaging\\Images\\test_images\\output_hugeSAM\\gfp_intensity_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "gfp_intensity_results_df = process_images(input_dir, output_dir, save_masks=True, save_masked_images=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtimes for 8 image pairs\n",
    "Base model: 1min 3.5s\n",
    "\n",
    "Large model: 2min 35.9s\n",
    "\n",
    "Huge model: 4min 53s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MoIm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
